This is the list of tasks of the second phase of the recruitment process for the intern position:

https://wiki.opnfv.org/display/DEV/Intern+Project%3A+kubernetes-sfc+scenario+investigation+and+prototype

Here is the list of tasks:
-----------------------------------------------------------------------------------------------------------------------------

#1 - Create a github account (if you don't have one yet) and fork this repo. We would like you to write down the answers of the questions we ask in the forked version of this text file. If the task requires some practical exercises, we would like you to write every command you use
-----------------------------------------------------------------------------------------------------------------------------
#2 - Set up the VPN to connect to our lab. To do so, read the instructions in the next link under the section "Using the VPN":

https://wiki.opnfv.org/display/INF/Lab-as-a-Service+at+the+UNH-IOL

Your username and password for the VPN were provided to you in the mail

1.Setup OpenVPN client.
2.Download laas_opnfv.ovpn configuration file from https://wiki.opnfv.org/display/INF/Lab-as-a-Service+at+the+UNH-IOL.

$ sudo apt-get install openvpn
$ sudo openvpn /home/niki/Downloads/laas_opnfv.ovpn

Authenticate using provided credentials.
-----------------------------------------------------------------------------------------------------------------------------


#3 - Connect to our server (10.10.100.21) using ssh with username opnfv and the same for password

$ ssh opnfv@10.10.100.21
Authenticate.
----------------------------------------------------------------------------------------------------------------------------

#4 - How would you ssh our server without having to write the password? If you know a way, apart from briefly explaining how, please try to do it. HINT: Restarting the sshd process (ssh server) is not needed

Assymetric Key Cryptography/ Encryption can be used to achieve this.
The keys are simply large numbers that have been paired together but are not identical (asymmetric). 
One key in the pair can be shared with everyone; it is called the public key. 
The other key in the pair is kept secret; it is called the private key. 

After keypair generation, we copy the public key in source to the user account in the destination.
Then we can connect to the destination from source using the private key of the key pair.
The SSH service will verify the private key in the source to the copied public key in the destination.
If both keys match, the connection will be established.

Step 1: Generate a ssh key pair in source machine.

The below command will generate a public/private rsa key pair in the server.
The public key will be saved as “id_rsa.pub” and private key  as “id_rsa” under .ssh folder in your home directory..

$ ssh-keygen –t rsa       # Generates keys using RSA Algorithm

Generating public/private rsa key pair.
Enter file in which to save the key (/home/niki/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/niki/.ssh/id_rsa.
Your public key has been saved in /home/niki/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:bOi9dgVBP+zyygme71UkmEKDTiY/aMfqTHQ1VantBdU niki@nikita-HP-ProBook-440-G1
The key's randomart image is:
+---[RSA 2048]----+
|       .ooo..o.. |
|    . +.o..=o   E|
|     O ...o+=..  |
|    + Bo .o..+.  |
|   o +..S .o...  |
|    o. o   oo.   |
|   +  . o  .o    |
|    o  ..=.+     |
|       .++*      |
+----[SHA256]-----+

Step 2: Copy the public key to destination

Now the public key has to be copied to the desired user account in the destination machine.
The key has to be added to a file named “authorized_keys” under ~/.ssh/authorized_keys in opnfv@10.10.100.21
If the above path doesn’t exist, we have to create it manually.
ssh opnfv@10.10.100.21 mkdir -p .ssh

$ ssh-add
$ ssh-copy-id opnfv@10.10.100.21
  OR
$ cat .ssh/id_rsa.pub | ssh opnfv@10.10.100.21 ssh root@192.168.122.4 'cat >> .ssh/authorized_keys'

Step 3: Connect from source using private key to destination

From source, run ssh username@destination. 
By default, ssh will pick the ‘id_rsa” private key file located in .ssh folder of user’s home directory.

If permission denied error for passwordless authentication arises,change permissions in destination.
set 700 (rwx——) for .ssh folder
set 600(rw——-) for authorized_keys file inside .ssh folder

$ ssh opnfv@10.10.100.21 "chmod 700 .ssh; chmod 600 .ssh/authorized_keys"

After correcting the permissions, we’re now able to establish passwordless ssh connection successfully.
-------------------------------------------------------------------------------------------------------------------------

#5 - ssh to your VM running in the server. The ip was provided in the mail
$ ssh root@192.168.122.4

-----------------------------------------------------------------------------------------------------------------------------

#6 - How would you ssh directly from your computer to the VM without having to use ssh twice (one to connect to the server and another one to connect to the VM)? If you know a way, apart from briefly explaining how, please try to do it.

This can be done by using ProxyCommand.
Here, ProxyCommand first is establishing an SSH connection to opnfv@10.10.100.21.  
Once the connection to the opnfv server is established, we need to make the next hop to the root@192.168.122.4.
The next part of ProxyCommand points STDIN at the target hostname and port using netcat.
Netcat(aka nc) is a standard UNIX utility for reading from and writing to network connections. 
STDOUT is also routed back for the return trip from the target host to the client’s machine, fulfilling the requirement for ProxyCommand to return response data on STDOUT.

First, you’ll want to ensure you have public key authentication properly configured, both on the opnfv@10.10.100.21 server as well as the root@192.168.122.4

1) The user enters ssh on localhost. This launches the parent ssh process
2) The parent ssh creates a child ssh with I/O redirected to pipes
3) The child ssh creates a connection to opnfv@10.10.100.21.
4) The sshd process on opnfv@10.10.100.21 creates a tcp connection to root@192.168.122.4:22
5) An ssh channel is added to existing ssh connection between localhost and opnfv@10.10.100.21.
6) Parent ssh writes the handshake data to the pipe, the child ssh reads it from the pipe, sends via the ssh channel to sshd on opnfv@10.10.100.21; sshd reads it and writes it to the socket connected to root@192.168.122.4. Similarly, the data is transmitted from root@192.168.122.4 to localhost.


The ssh program on a host receives its configuration from either the command line or from configuration files ~/.ssh/config and /etc/ssh/ssh_config.

Edit the $HOME/.ssh/config file using a text editor such as vi, enter:
$ vi ~/.ssh/config

Append the following to the config file:
------------------------------------------------------------------------------------------------------------------
Host host-a
  User opnfv                        #Defines the username for the SSH connection if different from local username
  Hostname 10.10.100.21             
                                    #IdentityFile ~/.ssh/id_rsa1 ---if private key is in another file, can be added
  
Host host-b
  User root                         #Defines the username for the SSH connection if different from local username 
  Hostname 192.168.122.4            
  ProxyCommand ssh host-a nc %h %p  #OR ProxyCommand ssh host-a -W %h:%p
  
                                    #IdentityFile ~/.ssh/id_rsa2 ---if private key is in another file, can be added
                                    # Port 22 default SSH port --can be added to override
------------------------------------------------------------------------------------------------------------------

$ ssh-add <(ssh opnfv@10.10.100.21 'cat .ssh/id_rsa')
$ ssh host-b 


-W command is a command-line option for SSH that enables “netcat mode.” This option was added in OpenSSH 5.4.
Earlier versions of SSH - use netcat (nc).
%h will be substituted by the host name to connect, %p by the port.

Connected to VM successfully.
-----------------------------------------------------------------------------------------------------------------------------

#7 - Install docker inside the VM

$ sudo apt-get update
$ sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo apt-key fingerprint 0EBFCD88
$ sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
$ sudo apt-get update
$ sudo apt-get install docker-ce
$ sudo docker run hello-world       #check whether docker is properly installed.


-----------------------------------------------------------------------------------------------------------------------------

#8 - Create 2 docker containers (use opensuse or ubuntu as their operating system) making sure that they don't stop when you leave them. From now on, we will call these containers: container1 and container2

$ docker run -td --name container1 ubuntu
$ docker run -td --name container2 ubuntu     #running in detached mode

-----------------------------------------------------------------------------------------------------------------------------

#9 - Make sure you can ping between both containers

Finding the ip addresses of containers
As they are connected to the default bridge network they can communicate with each other.

$ docker inspect -f '{{ .NetworkSettings.IPAddress }}' container1   # 172.17.0.2
$ docker inspect -f '{{ .NetworkSettings.IPAddress }}' container2   # 172.17.0.3

$ docker container exec -it container1 bash
# apt-get update
# apt-get install iputils-ping
# ping 172.17.0.3

-----------------------------------------------------------------------------------------------------------------------------

#10 - Explain what are the used networking protocols and the exchanged packets that appear in order for ping to be successful (e.g. container2 first sends a UDP packet with the IP of container 1...)

"No.","Time","Source","Destination","Protocol","Length","Info"
"1","0.000000000","172.17.0.3","172.17.0.2","ICMP","98","Echo (ping) request  id=0x0237, seq=1/256, ttl=64 (reply in 2)"
"2","0.000132013","172.17.0.2","172.17.0.3","ICMP","98","Echo (ping) reply    id=0x0237, seq=1/256, ttl=64 (request in 1)"
"3","1.023761117","172.17.0.3","172.17.0.2","ICMP","98","Echo (ping) request  id=0x0237, seq=2/512, ttl=64 (reply in 4)"
"4","1.023820736","172.17.0.2","172.17.0.3","ICMP","98","Echo (ping) reply    id=0x0237, seq=2/512, ttl=64 (request in 3)"
"5","2.047793314","172.17.0.3","172.17.0.2","ICMP","98","Echo (ping) request  id=0x0237, seq=3/768, ttl=64 (reply in 6)"
"6","2.047874612","172.17.0.2","172.17.0.3","ICMP","98","Echo (ping) reply    id=0x0237, seq=3/768, ttl=64 (request in 5)"
"7","3.071812836","172.17.0.3","172.17.0.2","ICMP","98","Echo (ping) request  id=0x0237, seq=4/1024, ttl=64 (reply in 8)"
"8","3.071917370","172.17.0.2","172.17.0.3","ICMP","98","Echo (ping) reply    id=0x0237, seq=4/1024, ttl=64 (request in 7)"
"9","4.095806036","172.17.0.3","172.17.0.2","ICMP","98","Echo (ping) request  id=0x0237, seq=5/1280, ttl=64 (reply in 10)"
"10","4.095896130","172.17.0.2","172.17.0.3","ICMP","98","Echo (ping) reply    id=0x0237, seq=5/1280, ttl=64 (request in 9)"
"11","5.023765744","02:42:ac:11:00:02","02:42:ac:11:00:03","ARP","42","Who has 172.17.0.3? Tell 172.17.0.2"
"12","5.023792604","02:42:ac:11:00:03","02:42:ac:11:00:02","ARP","42","172.17.0.3 is at 02:42:ac:11:00:03"
"13","5.119816801","172.17.0.3","172.17.0.2","ICMP","98","Echo (ping) request  id=0x0237, seq=6/1536, ttl=64 (reply in 14)"
"14","5.119915447","172.17.0.2","172.17.0.3","ICMP","98","Echo (ping) reply    id=0x0237, seq=6/1536, ttl=64 (request in 13)"

These are the packets captured using Wireshark when container1 was pinging container2.
ICMP(Internet Control Message Protocol) echo request and reply packets are exchanged.
ARP request is generated for resolving the destination MAC address.Once resolved ICMP request and reply messages are exchanged.
The exchange of packets indicates that the destination is reachable from the host through the network.
ICMP is technically a layer 3 protocol, so does not utilize UDP or TCP. 
ICMP is directly coupled with the IP Protocol.





----------------------------------------------------------------------------------------------------------------------------

#11 - Describe in detail what is the path the packets follow since they leave the interface inside container1 until they reach the interface inside container2

Linux bridge(docker0):  virtual implementation of a physical switch inside of the Linux kernel. 

Network namespace :  is an isolated network stack with its own collection of interfaces, routes and firewall rules. 

Virtual ethernet devices or veth : are interface that act as connections between two network namespaces. They have a single interface in each namespace. 

Iptables is a package filtering system, which acts as a layer 3/4 firewall and provide packet marking, masquerading and dropping features.

Bridge driver creates a Docker-managed Linux bridge on the Docker host. By default, all containers created on the same bridge can talk to each other.

When a docker container is created a veth pair is created. One veth is moved inside the container and renamed as eth0.
Other veth is in host namespace.
Attaches the host veth to the docker0 bridge. Port forwarding is configured in iptables.
Each container gets its own IP from the created subnet.
Containers can talk to each other and to the host via IP.
Outwards connectivity via IP forwarding and masquerading.
Just like a physical switch, a bridge can learn the MAC addresses reachable from each of it’s ports by monitoring ARP replies or by having static forwarding table entries configured.

Suppose A1 container on host A wants to communicate with another container B1 on the same host. A1 has the destination IP address of B1.
A1 would determine that the target IP address is inside it’s own subnet and decide to send out an ethernet frame addressed directly to the destination. It looks inside it’s ARP cache if it already has the MAC address corresponding to the destination IP.
If it doesn’t find a corresponding MAC in the ARP cache, it sends out an ARP request (basically saying “Who has this IP, please respond”).
This ARP request gets propagated through the veth pair to docker0. 
Docker0 floods it out all it’s virtual ports.
Once the ARP request reaches an interface with the correct IP address, this interface will send out an ARP reply to the MAC address originating the ARP request. 
This ARP reply travels through docker0 as well. Docker0 “takes a note” of this ARP reply and the virtual port it entered from — it now knows that the MAC address sending the ARP reply must be reachable from that virtual port. 
It reconfigures it’s forwarding table to send all future frames destined to that MAC address out the virtual port, where the ARP reply came from.
For example (A1 initiating communication with B1), after the ARP exchange, docker0 would know that the MAC 02:42:0a:00:00:03 is reachable from veth32.

-----------------------------------------------------------------------------------------------------------------------------

#12 - Imagine, container1 and container2 are in different VMs or servers. What do you think would change from what you described in #11?

Overlay approach is used in Docker for communication between containers on different hosts.
Encapsulates multiple networks over the physical networking.
Overlay network has it’s own namespace to provide isolation.
Vxlan interface is another special interface.
It allows us to create a “tunnel” between two or more Docker hosts and send ethernet frames from a container on one host to a container running on another host by encapsulating the frames inside UDP packets on the host network.
The Overlay driver creates an overlay network that may span over multiple Docker hosts. It uses both local Linux bridges and VXLAN to overlay inter-container communication over physical networks.

Source Container creates a L2 frame destined for MAC address of destination host.
The overlay network driver then encapsulates the frame with a VXLAN header, with the physical address of destination host, which he knows by knowing the state and location of every VXLAN tunnel endpoint through the control plane.
The packet is then sent and routed as a normal packet using the physical network.
Finally, the packet is received by destination host, decapsulated by the overlay network driver, and passed on to the above layer.

By creating and configuring a VXLAN interface, you are telling the kernel to listen on UDP port 4789 in the host network namespace for UDP datagrams containing VXLAN encapsulated ethernet frames from another host. 
For containers residing on remote Docker hosts, the vxlan1 interface sends out ARP replies on their behalf. 
Docker0 “learns” from these ARP replies that it should forward frames destined for remote containers to vxlan1.  
Which containers are on which Docker hosts is defined in the forwarding table of vxlan1.
Entries are updated in the forwarding tables of all vxlan interfaces (on all Docker hosts in an overlay network) whenever a container is created or removed. 

-----------------------------------------------------------------------------------------------------------------------------

#13 - Push this file with the answers to your private github repository
-----------------------------------------------------------------------------------------------------------------------------

#14 - Send an email to Dimitris and Manuel with a link to the document in your github repo
-----------------------------------------------------------------------------------------------------------------------------


Please contact Dimitris or Manuel if you get stuck in any of the steps
